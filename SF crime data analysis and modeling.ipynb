{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SF crime data analysis and modeling\n",
    "### Data Source: https://www.kaggle.com/c/sf-crime/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "from pyspark.sql import Row \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "#from ggplot import *\n",
    "import warnings\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing PySpark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# #Spark Config\n",
    "conf = SparkConf().setAppName(\"sample_app\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from the data storage\n",
    "# please upload your data into databricks community at first. \n",
    "crime_data_lines = sc.textFile('sf_data.csv')\n",
    "#prepare data \n",
    "df_crimes = crime_data_lines.map(lambda line: [x.strip('\"') for x in next(reader([line]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dates',\n",
       " 'Category',\n",
       " 'Descript',\n",
       " 'DayOfWeek',\n",
       " 'PdDistrict',\n",
       " 'Resolution',\n",
       " 'Address',\n",
       " 'X',\n",
       " 'Y']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get header\n",
    "header = df_crimes.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the first line of data\n",
    "crimes = df_crimes.filter(lambda x: x != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2015-05-13 23:53:00',\n",
       "  'WARRANTS',\n",
       "  'WARRANT ARREST',\n",
       "  'Wednesday',\n",
       "  'NORTHERN',\n",
       "  'ARREST, BOOKED',\n",
       "  'OAK ST / LAGUNA ST',\n",
       "  '-122.425891675136',\n",
       "  '37.7745985956747'],\n",
       " ['2015-05-13 23:53:00',\n",
       "  'OTHER OFFENSES',\n",
       "  'TRAFFIC VIOLATION ARREST',\n",
       "  'Wednesday',\n",
       "  'NORTHERN',\n",
       "  'ARREST, BOOKED',\n",
       "  'OAK ST / LAGUNA ST',\n",
       "  '-122.425891675136',\n",
       "  '37.7745985956747'],\n",
       " ['2015-05-13 23:33:00',\n",
       "  'OTHER OFFENSES',\n",
       "  'TRAFFIC VIOLATION ARREST',\n",
       "  'Wednesday',\n",
       "  'NORTHERN',\n",
       "  'ARREST, BOOKED',\n",
       "  'VANNESS AV / GREENWICH ST',\n",
       "  '-122.42436302145',\n",
       "  '37.8004143219856'],\n",
       " ['2015-05-13 23:30:00',\n",
       "  'LARCENY/THEFT',\n",
       "  'GRAND THEFT FROM LOCKED AUTO',\n",
       "  'Wednesday',\n",
       "  'NORTHERN',\n",
       "  'NONE',\n",
       "  '1500 Block of LOMBARD ST',\n",
       "  '-122.42699532676599',\n",
       "  '37.80087263276921'],\n",
       " ['2015-05-13 23:30:00',\n",
       "  'LARCENY/THEFT',\n",
       "  'GRAND THEFT FROM LOCKED AUTO',\n",
       "  'Wednesday',\n",
       "  'PARK',\n",
       "  'NONE',\n",
       "  '100 Block of BRODERICK ST',\n",
       "  '-122.438737622757',\n",
       "  '37.771541172057795']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the first line of data\n",
    "display(crimes.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "878049"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the total number of data \n",
    "crimes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solove big data issues via Spark\n",
    "###### approach 1: use RDD (not recommend)\n",
    "###### approach 2: use Dataframe, register the RDD to a dataframe (recommend for DE)\n",
    "###### approach 3: use SQL (recomend for data analysis or DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"crime analysis\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Dates: string, Category: string, Descript: string, DayOfWeek: string, PdDistrict: string, Resolution: string, Address: string, X: string, Y: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_opt1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"sf_data.csv\")\n",
    "display(df_opt1)\n",
    "df_opt1.createOrReplaceTempView(\"sf_crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "|              Dates|      Category|            Descript|DayOfWeek|PdDistrict|    Resolution|             Address|                  X|                 Y|\n",
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "|2015-05-13 23:53:00|      WARRANTS|      WARRANT ARREST|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|  -122.425891675136|  37.7745985956747|\n",
      "|2015-05-13 23:53:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|  -122.425891675136|  37.7745985956747|\n",
      "|2015-05-13 23:33:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|VANNESS AV / GREE...|   -122.42436302145|  37.8004143219856|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  NORTHERN|          NONE|1500 Block of LOM...|-122.42699532676599| 37.80087263276921|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|      PARK|          NONE|100 Block of BROD...|  -122.438737622757|37.771541172057795|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday| INGLESIDE|          NONE| 0 Block of TEDDY AV|-122.40325236121201|   37.713430704116|\n",
      "|2015-05-13 23:30:00| VEHICLE THEFT|   STOLEN AUTOMOBILE|Wednesday| INGLESIDE|          NONE| AVALON AV / PERU AV|  -122.423326976668|  37.7251380403778|\n",
      "|2015-05-13 23:30:00| VEHICLE THEFT|   STOLEN AUTOMOBILE|Wednesday|   BAYVIEW|          NONE|KIRKWOOD AV / DON...|  -122.371274317441|  37.7275640719518|\n",
      "|2015-05-13 23:00:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  RICHMOND|          NONE|600 Block of 47TH AV|  -122.508194031117|37.776601260681204|\n",
      "|2015-05-13 23:00:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|   CENTRAL|          NONE|JEFFERSON ST / LE...|  -122.419087676747|  37.8078015516515|\n",
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_opt1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def createRow(keys, values):\n",
    "  assert len(keys) == len(values)\n",
    "  mapped = dict(zip(keys, values))\n",
    "  return Row(**mapped)\n",
    "\n",
    "rdd_rows = crimes.map(lambda x: createRow(header, x))\n",
    "\n",
    "df_opt2 = spark.createDataFrame(rdd_rows)\n",
    "df_opt2.createOrReplaceTempView(\"sf_crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Address: string, Category: string, Dates: string, DayOfWeek: string, Descript: string, PdDistrict: string, Resolution: string, X: string, Y: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_opt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+-------------------+---------+--------------------+----------+--------------+-------------------+------------------+\n",
      "|             Address|      Category|              Dates|DayOfWeek|            Descript|PdDistrict|    Resolution|                  X|                 Y|\n",
      "+--------------------+--------------+-------------------+---------+--------------------+----------+--------------+-------------------+------------------+\n",
      "|  OAK ST / LAGUNA ST|      WARRANTS|2015-05-13 23:53:00|Wednesday|      WARRANT ARREST|  NORTHERN|ARREST, BOOKED|  -122.425891675136|  37.7745985956747|\n",
      "|  OAK ST / LAGUNA ST|OTHER OFFENSES|2015-05-13 23:53:00|Wednesday|TRAFFIC VIOLATION...|  NORTHERN|ARREST, BOOKED|  -122.425891675136|  37.7745985956747|\n",
      "|VANNESS AV / GREE...|OTHER OFFENSES|2015-05-13 23:33:00|Wednesday|TRAFFIC VIOLATION...|  NORTHERN|ARREST, BOOKED|   -122.42436302145|  37.8004143219856|\n",
      "|1500 Block of LOM...| LARCENY/THEFT|2015-05-13 23:30:00|Wednesday|GRAND THEFT FROM ...|  NORTHERN|          NONE|-122.42699532676599| 37.80087263276921|\n",
      "|100 Block of BROD...| LARCENY/THEFT|2015-05-13 23:30:00|Wednesday|GRAND THEFT FROM ...|      PARK|          NONE|  -122.438737622757|37.771541172057795|\n",
      "| 0 Block of TEDDY AV| LARCENY/THEFT|2015-05-13 23:30:00|Wednesday|GRAND THEFT FROM ...| INGLESIDE|          NONE|-122.40325236121201|   37.713430704116|\n",
      "| AVALON AV / PERU AV| VEHICLE THEFT|2015-05-13 23:30:00|Wednesday|   STOLEN AUTOMOBILE| INGLESIDE|          NONE|  -122.423326976668|  37.7251380403778|\n",
      "|KIRKWOOD AV / DON...| VEHICLE THEFT|2015-05-13 23:30:00|Wednesday|   STOLEN AUTOMOBILE|   BAYVIEW|          NONE|  -122.371274317441|  37.7275640719518|\n",
      "|600 Block of 47TH AV| LARCENY/THEFT|2015-05-13 23:00:00|Wednesday|GRAND THEFT FROM ...|  RICHMOND|          NONE|  -122.508194031117|37.776601260681204|\n",
      "|JEFFERSON ST / LE...| LARCENY/THEFT|2015-05-13 23:00:00|Wednesday|GRAND THEFT FROM ...|   CENTRAL|          NONE|  -122.419087676747|  37.8078015516515|\n",
      "+--------------------+--------------+-------------------+---------+--------------------+----------+--------------+-------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_opt2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Dates: string, Category: string, Descript: string, DayOfWeek: string, PdDistrict: string, Resolution: string, Address: string, X: string, Y: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_opt3 = crimes.toDF(['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y'])\n",
    "display(df_opt3)\n",
    "df_opt3.createOrReplaceTempView(\"sf_crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "|              Dates|      Category|            Descript|DayOfWeek|PdDistrict|    Resolution|             Address|                  X|                 Y|\n",
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "|2015-05-13 23:53:00|      WARRANTS|      WARRANT ARREST|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|  -122.425891675136|  37.7745985956747|\n",
      "|2015-05-13 23:53:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|  -122.425891675136|  37.7745985956747|\n",
      "|2015-05-13 23:33:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|VANNESS AV / GREE...|   -122.42436302145|  37.8004143219856|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  NORTHERN|          NONE|1500 Block of LOM...|-122.42699532676599| 37.80087263276921|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|      PARK|          NONE|100 Block of BROD...|  -122.438737622757|37.771541172057795|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday| INGLESIDE|          NONE| 0 Block of TEDDY AV|-122.40325236121201|   37.713430704116|\n",
      "|2015-05-13 23:30:00| VEHICLE THEFT|   STOLEN AUTOMOBILE|Wednesday| INGLESIDE|          NONE| AVALON AV / PERU AV|  -122.423326976668|  37.7251380403778|\n",
      "|2015-05-13 23:30:00| VEHICLE THEFT|   STOLEN AUTOMOBILE|Wednesday|   BAYVIEW|          NONE|KIRKWOOD AV / DON...|  -122.371274317441|  37.7275640719518|\n",
      "|2015-05-13 23:00:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  RICHMOND|          NONE|600 Block of 47TH AV|  -122.508194031117|37.776601260681204|\n",
      "|2015-05-13 23:00:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|   CENTRAL|          NONE|JEFFERSON ST / LE...|  -122.419087676747|  37.8078015516515|\n",
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_opt3.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 question (OLAP):\n",
    "#####  Write a Spark program that counts the number of crimes for different category.\n",
    "Below are some example codes to demonstrate the way to use Spark RDD, DF, and SQL to work with big data. You can follow this example to finish other questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2019/06/30', 578),\n",
       " ('2018/08/01', 556),\n",
       " ('2018/08/24', 532),\n",
       " ('2019/10/02', 527),\n",
       " ('2019/04/03', 521),\n",
       " ('2018/08/10', 517),\n",
       " ('2019/09/02', 515),\n",
       " ('2019/08/10', 514),\n",
       " ('2018/04/25', 514),\n",
       " ('2019/10/04', 513),\n",
       " ('2019/08/07', 512),\n",
       " ('2019/02/01', 512),\n",
       " ('2019/08/30', 507),\n",
       " ('2018/04/11', 504),\n",
       " ('2018/04/13', 504),\n",
       " ('2018/07/27', 504),\n",
       " ('2019/10/28', 501),\n",
       " ('2019/07/26', 501),\n",
       " ('2019/02/20', 501),\n",
       " ('2018/08/03', 499),\n",
       " ('2018/08/14', 498),\n",
       " ('2018/11/09', 497),\n",
       " ('2018/01/01', 496),\n",
       " ('2019/09/06', 494),\n",
       " ('2019/12/04', 494),\n",
       " ('2019/07/24', 491),\n",
       " ('2019/07/01', 489),\n",
       " ('2018/08/25', 489),\n",
       " ('2018/07/21', 489),\n",
       " ('2018/10/15', 488),\n",
       " ('2018/05/08', 488),\n",
       " ('2018/01/26', 488),\n",
       " ('2018/07/20', 486),\n",
       " ('2018/07/13', 486),\n",
       " ('2018/04/20', 486),\n",
       " ('2018/06/01', 485),\n",
       " ('2019/10/25', 485),\n",
       " ('2018/05/18', 485),\n",
       " ('2019/05/24', 484),\n",
       " ('2018/06/24', 484),\n",
       " ('2018/09/08', 484),\n",
       " ('2019/08/14', 484),\n",
       " ('2018/07/17', 482),\n",
       " ('2019/08/01', 482),\n",
       " ('2018/10/01', 482),\n",
       " ('2018/05/04', 481),\n",
       " ('2018/01/19', 481),\n",
       " ('2019/01/19', 479),\n",
       " ('2019/07/08', 479),\n",
       " ('2018/11/01', 478),\n",
       " ('2018/08/27', 478),\n",
       " ('2019/07/02', 477),\n",
       " ('2019/10/16', 477),\n",
       " ('2018/10/04', 477),\n",
       " ('2019/11/01', 476),\n",
       " ('2018/07/18', 475),\n",
       " ('2018/03/23', 475),\n",
       " ('2019/09/18', 475),\n",
       " ('2018/08/05', 474),\n",
       " ('2018/01/04', 472),\n",
       " ('2018/07/02', 472),\n",
       " ('2018/02/08', 472),\n",
       " ('2018/09/19', 471),\n",
       " ('2018/10/31', 471),\n",
       " ('2019/09/28', 471),\n",
       " ('2018/08/11', 471),\n",
       " ('2018/07/31', 470),\n",
       " ('2018/07/05', 470),\n",
       " ('2018/06/22', 469),\n",
       " ('2018/08/07', 469),\n",
       " ('2019/04/10', 469),\n",
       " ('2018/07/25', 469),\n",
       " ('2018/09/21', 469),\n",
       " ('2018/03/01', 468),\n",
       " ('2018/08/21', 468),\n",
       " ('2018/09/12', 468),\n",
       " ('2019/05/01', 467),\n",
       " ('2018/05/01', 467),\n",
       " ('2018/01/12', 467),\n",
       " ('2018/09/14', 467),\n",
       " ('2019/08/21', 467),\n",
       " ('2018/02/01', 466),\n",
       " ('2018/07/24', 466),\n",
       " ('2018/02/09', 465),\n",
       " ('2019/01/25', 465),\n",
       " ('2019/10/01', 464),\n",
       " ('2018/01/31', 464),\n",
       " ('2019/10/18', 463),\n",
       " ('2019/08/20', 463),\n",
       " ('2018/12/07', 463),\n",
       " ('2018/10/20', 462),\n",
       " ('2019/02/15', 462),\n",
       " ('2018/10/10', 459),\n",
       " ('2019/07/29', 458),\n",
       " ('2019/07/22', 458),\n",
       " ('2019/09/29', 458),\n",
       " ('2018/07/30', 458),\n",
       " ('2018/08/02', 458),\n",
       " ('2018/02/24', 458),\n",
       " ('2019/09/26', 457),\n",
       " ('2018/03/16', 457),\n",
       " ('2018/12/20', 457),\n",
       " ('2018/09/20', 457),\n",
       " ('2019/08/22', 456),\n",
       " ('2018/08/31', 456),\n",
       " ('2018/01/27', 456),\n",
       " ('2019/10/26', 456),\n",
       " ('2018/09/01', 455),\n",
       " ('2019/09/01', 455),\n",
       " ('2018/03/30', 455),\n",
       " ('2018/08/15', 455),\n",
       " ('2019/08/09', 455),\n",
       " ('2018/08/29', 455),\n",
       " ('2018/04/18', 454),\n",
       " ('2018/01/05', 454),\n",
       " ('2018/02/20', 454),\n",
       " ('2018/01/23', 453),\n",
       " ('2019/10/19', 453),\n",
       " ('2018/02/10', 453),\n",
       " ('2018/01/06', 453),\n",
       " ('2018/06/29', 453),\n",
       " ('2018/09/28', 453),\n",
       " ('2019/08/15', 453),\n",
       " ('2019/09/27', 452),\n",
       " ('2019/02/25', 452),\n",
       " ('2019/07/28', 452),\n",
       " ('2018/11/10', 452),\n",
       " ('2018/07/03', 452),\n",
       " ('2018/01/22', 452),\n",
       " ('2018/07/19', 452),\n",
       " ('2018/12/19', 451),\n",
       " ('2019/11/20', 451),\n",
       " ('2018/02/17', 451),\n",
       " ('2019/10/06', 451),\n",
       " ('2019/05/22', 451),\n",
       " ('2019/09/20', 450),\n",
       " ('2018/07/23', 450),\n",
       " ('2019/08/19', 450),\n",
       " ('2018/01/25', 450),\n",
       " ('2018/03/27', 449),\n",
       " ('2018/07/14', 449),\n",
       " ('2018/09/15', 449),\n",
       " ('2019/08/02', 449),\n",
       " ('2018/06/12', 449),\n",
       " ('2018/01/30', 448),\n",
       " ('2018/02/28', 448),\n",
       " ('2018/10/06', 448),\n",
       " ('2019/09/14', 448),\n",
       " ('2018/04/27', 448),\n",
       " ('2019/10/30', 448),\n",
       " ('2018/02/22', 447),\n",
       " ('2019/07/12', 447),\n",
       " ('2018/07/16', 447),\n",
       " ('2019/09/25', 447),\n",
       " ('2018/09/05', 447),\n",
       " ('2018/08/06', 447),\n",
       " ('2019/11/15', 447),\n",
       " ('2019/08/29', 446),\n",
       " ('2018/06/28', 446),\n",
       " ('2018/06/23', 446),\n",
       " ('2019/09/03', 446),\n",
       " ('2018/10/11', 445),\n",
       " ('2018/03/28', 445),\n",
       " ('2018/10/03', 445),\n",
       " ('2019/10/05', 444),\n",
       " ('2018/10/07', 444),\n",
       " ('2019/01/01', 444),\n",
       " ('2019/08/23', 444),\n",
       " ('2018/05/12', 444),\n",
       " ('2018/09/22', 444),\n",
       " ('2018/02/15', 444),\n",
       " ('2018/04/19', 443),\n",
       " ('2018/04/04', 443),\n",
       " ('2019/08/26', 442),\n",
       " ('2019/10/10', 442),\n",
       " ('2018/10/16', 442),\n",
       " ('2018/08/22', 442),\n",
       " ('2019/07/03', 442),\n",
       " ('2018/12/18', 442),\n",
       " ('2019/10/11', 441),\n",
       " ('2018/01/13', 441),\n",
       " ('2019/11/22', 441),\n",
       " ('2018/12/28', 441),\n",
       " ('2018/10/22', 441),\n",
       " ('2018/02/06', 441),\n",
       " ('2018/10/05', 440),\n",
       " ('2018/09/07', 440),\n",
       " ('2019/04/24', 440),\n",
       " ('2019/04/11', 440),\n",
       " ('2018/02/21', 440),\n",
       " ('2018/05/23', 439),\n",
       " ('2018/11/14', 439),\n",
       " ('2018/10/17', 439),\n",
       " ('2018/07/11', 438),\n",
       " ('2019/03/08', 438),\n",
       " ('2018/07/01', 438),\n",
       " ('2018/09/18', 438),\n",
       " ('2018/02/02', 438),\n",
       " ('2019/06/29', 438),\n",
       " ('2019/06/07', 437),\n",
       " ('2019/07/13', 437),\n",
       " ('2018/12/23', 437),\n",
       " ('2018/05/10', 437),\n",
       " ('2018/06/16', 437),\n",
       " ('2019/02/21', 437),\n",
       " ('2018/05/05', 436),\n",
       " ('2018/06/25', 436),\n",
       " ('2019/06/19', 436),\n",
       " ('2018/09/25', 436),\n",
       " ('2018/08/28', 436),\n",
       " ('2018/02/14', 435),\n",
       " ('2018/07/06', 435),\n",
       " ('2018/09/03', 435),\n",
       " ('2019/10/23', 434),\n",
       " ('2018/07/26', 434),\n",
       " ('2018/10/13', 433),\n",
       " ('2018/11/13', 433),\n",
       " ('2019/07/17', 433),\n",
       " ('2019/08/05', 432),\n",
       " ('2019/02/22', 432),\n",
       " ('2018/07/29', 432),\n",
       " ('2018/07/10', 432),\n",
       " ('2019/08/13', 432),\n",
       " ('2018/10/23', 431),\n",
       " ('2018/06/19', 431),\n",
       " ('2019/05/03', 431),\n",
       " ('2019/06/12', 431),\n",
       " ('2018/04/26', 431),\n",
       " ('2019/04/04', 431),\n",
       " ('2019/12/06', 431),\n",
       " ('2018/01/03', 430),\n",
       " ('2018/02/23', 430),\n",
       " ('2019/08/06', 430),\n",
       " ('2018/08/12', 430),\n",
       " ('2019/05/31', 429),\n",
       " ('2018/09/24', 429),\n",
       " ('2019/03/12', 429),\n",
       " ('2018/05/16', 429),\n",
       " ('2019/06/01', 429),\n",
       " ('2018/09/02', 429),\n",
       " ('2019/08/25', 429),\n",
       " ('2018/06/08', 429),\n",
       " ('2019/07/31', 429),\n",
       " ('2019/03/21', 428),\n",
       " ('2019/03/06', 428),\n",
       " ('2018/03/09', 428),\n",
       " ('2018/10/21', 428),\n",
       " ('2018/08/20', 428),\n",
       " ('2018/10/27', 428),\n",
       " ('2019/07/23', 428),\n",
       " ('2018/05/15', 428),\n",
       " ('2019/05/06', 428),\n",
       " ('2019/08/18', 427),\n",
       " ('2019/01/31', 427),\n",
       " ('2019/11/13', 427),\n",
       " ('2019/10/22', 427),\n",
       " ('2019/09/19', 427),\n",
       " ('2019/10/29', 427),\n",
       " ('2018/07/07', 427),\n",
       " ('2018/10/19', 427),\n",
       " ('2019/04/08', 426),\n",
       " ('2019/11/04', 426),\n",
       " ('2019/04/19', 426),\n",
       " ('2018/03/31', 426),\n",
       " ('2018/12/22', 426),\n",
       " ('2018/07/28', 426),\n",
       " ('2018/10/12', 425),\n",
       " ('2019/08/31', 425),\n",
       " ('2018/08/08', 425),\n",
       " ('2018/12/03', 425),\n",
       " ('2019/03/01', 424),\n",
       " ('2019/02/02', 424),\n",
       " ('2018/10/26', 424),\n",
       " ('2019/11/12', 423),\n",
       " ('2018/11/03', 423),\n",
       " ('2019/08/24', 423),\n",
       " ('2018/05/02', 423),\n",
       " ('2018/01/29', 423),\n",
       " ('2019/10/09', 422),\n",
       " ('2018/04/14', 422),\n",
       " ('2019/11/14', 422),\n",
       " ('2019/11/21', 422),\n",
       " ('2019/03/29', 422),\n",
       " ('2019/08/12', 422),\n",
       " ('2019/07/16', 421),\n",
       " ('2019/05/30', 421),\n",
       " ('2018/04/28', 421),\n",
       " ('2018/01/09', 421),\n",
       " ('2019/12/03', 421),\n",
       " ('2019/08/08', 421),\n",
       " ('2018/06/26', 421),\n",
       " ('2019/11/09', 421),\n",
       " ('2018/03/14', 421),\n",
       " ('2019/05/05', 421),\n",
       " ('2018/01/11', 420),\n",
       " ('2018/11/12', 420),\n",
       " ('2019/07/10', 420),\n",
       " ('2018/06/30', 420),\n",
       " ('2019/09/11', 420),\n",
       " ('2018/02/07', 420),\n",
       " ('2018/05/31', 419),\n",
       " ('2018/01/10', 419),\n",
       " ('2019/09/13', 419),\n",
       " ('2019/06/11', 419),\n",
       " ('2019/06/03', 419),\n",
       " ('2018/04/10', 419),\n",
       " ('2018/08/13', 419),\n",
       " ('2018/12/14', 419),\n",
       " ('2018/05/20', 419),\n",
       " ('2018/03/10', 418),\n",
       " ('2019/03/16', 418),\n",
       " ('2018/02/25', 418),\n",
       " ('2018/04/16', 418),\n",
       " ('2019/08/17', 418),\n",
       " ('2018/08/30', 418),\n",
       " ('2019/09/24', 417),\n",
       " ('2018/05/17', 417),\n",
       " ('2019/10/20', 417),\n",
       " ('2019/08/28', 417),\n",
       " ('2018/05/11', 417),\n",
       " ('2019/11/08', 416),\n",
       " ('2018/01/24', 416),\n",
       " ('2018/09/04', 416),\n",
       " ('2018/04/06', 416),\n",
       " ('2018/12/17', 416),\n",
       " ('2018/06/27', 416),\n",
       " ('2018/05/24', 416),\n",
       " ('2019/10/21', 415),\n",
       " ('2019/02/08', 415),\n",
       " ('2019/04/17', 415),\n",
       " ('2019/11/07', 415),\n",
       " ('2018/01/15', 415),\n",
       " ('2018/04/05', 415),\n",
       " ('2018/02/12', 415),\n",
       " ('2018/08/23', 415),\n",
       " ('2018/12/27', 415),\n",
       " ('2019/04/07', 415),\n",
       " ('2019/06/28', 414),\n",
       " ('2019/06/15', 414),\n",
       " ('2018/11/29', 414),\n",
       " ('2019/11/02', 414),\n",
       " ('2018/01/18', 413),\n",
       " ('2019/09/09', 413),\n",
       " ('2018/12/05', 413),\n",
       " ('2019/01/23', 413),\n",
       " ('2019/10/03', 413),\n",
       " ('2018/05/09', 413),\n",
       " ('2018/02/03', 412),\n",
       " ('2018/06/15', 412),\n",
       " ('2018/05/26', 412),\n",
       " ('2018/12/29', 412),\n",
       " ('2019/03/22', 412),\n",
       " ('2018/09/29', 412),\n",
       " ('2019/01/04', 412),\n",
       " ('2019/08/11', 411),\n",
       " ('2018/12/08', 411),\n",
       " ('2019/10/17', 411),\n",
       " ('2019/04/01', 411),\n",
       " ('2018/07/12', 411),\n",
       " ('2019/09/30', 411),\n",
       " ('2018/11/26', 411),\n",
       " ('2018/11/20', 410),\n",
       " ('2018/11/08', 410),\n",
       " ('2018/11/30', 410),\n",
       " ('2019/12/09', 410),\n",
       " ('2018/11/28', 409),\n",
       " ('2019/03/28', 409),\n",
       " ('2018/08/16', 409),\n",
       " ('2019/03/30', 409),\n",
       " ('2019/06/26', 409),\n",
       " ('2018/08/17', 409),\n",
       " ('2019/02/28', 409),\n",
       " ('2019/09/17', 409),\n",
       " ('2019/06/04', 409),\n",
       " ('2019/10/24', 409),\n",
       " ('2019/07/05', 409),\n",
       " ('2019/07/25', 409),\n",
       " ('2019/07/27', 408),\n",
       " ('2018/05/22', 408),\n",
       " ('2019/12/13', 408),\n",
       " ('2018/12/01', 408),\n",
       " ('2019/09/21', 408),\n",
       " ('2019/02/19', 408),\n",
       " ('2019/06/24', 408),\n",
       " ('2018/04/30', 408),\n",
       " ('2019/05/28', 407),\n",
       " ('2019/07/15', 407),\n",
       " ('2018/08/04', 407),\n",
       " ('2018/10/25', 407),\n",
       " ('2019/01/14', 406),\n",
       " ('2018/11/19', 406),\n",
       " ('2019/10/14', 406),\n",
       " ('2019/11/06', 406),\n",
       " ('2018/05/27', 406),\n",
       " ('2018/10/18', 406),\n",
       " ('2018/03/13', 406),\n",
       " ('2019/12/05', 406),\n",
       " ('2018/11/11', 405),\n",
       " ('2018/04/02', 405),\n",
       " ('2018/05/28', 405),\n",
       " ('2018/07/15', 405),\n",
       " ('2018/09/23', 405),\n",
       " ('2019/10/12', 405),\n",
       " ('2019/01/21', 405),\n",
       " ('2019/09/16', 405),\n",
       " ('2019/05/20', 405),\n",
       " ('2018/09/11', 405),\n",
       " ('2019/05/07', 404),\n",
       " ('2018/02/27', 404),\n",
       " ('2018/12/04', 404),\n",
       " ('2018/01/17', 404),\n",
       " ('2018/04/09', 404),\n",
       " ('2018/06/13', 404),\n",
       " ('2018/03/03', 404),\n",
       " ('2018/03/22', 403),\n",
       " ('2018/06/09', 403),\n",
       " ('2018/05/25', 403),\n",
       " ('2019/01/22', 403),\n",
       " ('2019/09/05', 403),\n",
       " ('2019/07/19', 403),\n",
       " ('2018/06/06', 403),\n",
       " ('2019/06/21', 403),\n",
       " ('2018/03/19', 402),\n",
       " ('2018/12/12', 402),\n",
       " ('2019/05/17', 402),\n",
       " ('2018/06/14', 402),\n",
       " ('2018/10/28', 402),\n",
       " ('2018/04/01', 401),\n",
       " ('2019/05/26', 401),\n",
       " ('2018/11/02', 401),\n",
       " ('2018/06/20', 401),\n",
       " ('2019/04/18', 401),\n",
       " ('2019/08/16', 401),\n",
       " ('2018/10/30', 401),\n",
       " ('2018/05/19', 401),\n",
       " ('2019/04/05', 401),\n",
       " ('2019/07/20', 401),\n",
       " ('2019/09/10', 401),\n",
       " ('2018/01/28', 401),\n",
       " ('2018/12/21', 401),\n",
       " ('2018/11/05', 401),\n",
       " ('2018/02/16', 400),\n",
       " ('2018/12/09', 400),\n",
       " ('2019/05/25', 400),\n",
       " ('2018/03/15', 400),\n",
       " ('2019/03/19', 400),\n",
       " ('2018/03/17', 400),\n",
       " ('2018/08/18', 399),\n",
       " ('2018/09/17', 399),\n",
       " ('2018/11/07', 399),\n",
       " ('2018/09/16', 399),\n",
       " ('2019/02/12', 399),\n",
       " ('2018/04/17', 398),\n",
       " ('2018/02/26', 398),\n",
       " ('2018/09/27', 398),\n",
       " ('2019/11/17', 398),\n",
       " ('2018/01/02', 398),\n",
       " ('2019/01/09', 398),\n",
       " ('2018/07/04', 398),\n",
       " ('2018/05/14', 398),\n",
       " ('2019/10/08', 398),\n",
       " ('2019/07/30', 398),\n",
       " ('2018/01/14', 398),\n",
       " ('2019/05/13', 398),\n",
       " ('2019/08/03', 397),\n",
       " ('2019/03/15', 397),\n",
       " ('2018/07/22', 397),\n",
       " ('2018/03/04', 397),\n",
       " ('2019/05/15', 397),\n",
       " ('2018/03/18', 397),\n",
       " ('2019/01/18', 397),\n",
       " ('2018/04/24', 397),\n",
       " ('2019/03/07', 397),\n",
       " ('2019/06/25', 397),\n",
       " ('2019/05/14', 396),\n",
       " ('2019/06/10', 396),\n",
       " ('2019/01/08', 396),\n",
       " ('2019/06/20', 396),\n",
       " ('2018/11/24', 396),\n",
       " ('2019/02/05', 396),\n",
       " ('2019/01/30', 396),\n",
       " ('2018/10/08', 395),\n",
       " ('2019/03/18', 395),\n",
       " ('2018/12/15', 395),\n",
       " ('2018/03/08', 394),\n",
       " ('2018/06/05', 394),\n",
       " ('2019/01/29', 394),\n",
       " ('2018/12/31', 394),\n",
       " ('2018/03/24', 394),\n",
       " ('2019/11/05', 394),\n",
       " ('2018/12/26', 394),\n",
       " ('2019/11/25', 394),\n",
       " ('2018/09/10', 393),\n",
       " ('2018/08/09', 393),\n",
       " ('2019/06/18', 393),\n",
       " ('2018/01/07', 393),\n",
       " ('2018/10/09', 392),\n",
       " ('2018/03/12', 392),\n",
       " ('2018/05/21', 392),\n",
       " ('2018/06/11', 392),\n",
       " ('2019/04/15', 392),\n",
       " ('2018/03/07', 392),\n",
       " ('2018/09/09', 391),\n",
       " ('2019/05/19', 391),\n",
       " ('2019/03/26', 391),\n",
       " ('2019/01/05', 391),\n",
       " ('2019/07/21', 391),\n",
       " ('2018/03/02', 391),\n",
       " ('2019/10/15', 390),\n",
       " ('2018/05/30', 390),\n",
       " ('2018/02/05', 390),\n",
       " ('2019/01/10', 390),\n",
       " ('2019/09/07', 390),\n",
       " ('2019/06/05', 390),\n",
       " ('2018/05/06', 390),\n",
       " ('2019/09/23', 389),\n",
       " ('2018/02/18', 389),\n",
       " ('2019/06/14', 389),\n",
       " ('2019/11/19', 389),\n",
       " ('2019/05/18', 389),\n",
       " ('2019/04/27', 388),\n",
       " ('2018/05/03', 388),\n",
       " ('2018/06/02', 388),\n",
       " ('2019/06/06', 388),\n",
       " ('2018/03/25', 388),\n",
       " ('2019/04/23', 388),\n",
       " ('2019/11/10', 387),\n",
       " ('2018/10/02', 387),\n",
       " ('2019/02/09', 387),\n",
       " ('2018/11/23', 387),\n",
       " ('2019/05/10', 387),\n",
       " ('2019/05/29', 386),\n",
       " ('2018/11/21', 386),\n",
       " ('2019/09/12', 386),\n",
       " ('2019/11/11', 386),\n",
       " ('2019/09/15', 386),\n",
       " ('2019/11/16', 386),\n",
       " ('2018/09/26', 386),\n",
       " ('2019/06/27', 385),\n",
       " ('2019/07/07', 385),\n",
       " ('2019/12/11', 385),\n",
       " ('2019/04/22', 384),\n",
       " ('2018/12/13', 384),\n",
       " ('2019/01/11', 384),\n",
       " ('2019/08/27', 384),\n",
       " ('2018/01/20', 384),\n",
       " ('2018/04/03', 383),\n",
       " ('2019/06/17', 383),\n",
       " ('2019/07/11', 383),\n",
       " ('2018/04/07', 383),\n",
       " ('2019/01/26', 383),\n",
       " ('2018/06/10', 383),\n",
       " ('2018/11/15', 383),\n",
       " ('2019/04/16', 383),\n",
       " ('2019/09/04', 383),\n",
       " ('2019/12/12', 383),\n",
       " ('2018/03/29', 382),\n",
       " ('2018/12/24', 382),\n",
       " ('2018/03/20', 382),\n",
       " ('2019/09/22', 381),\n",
       " ('2018/07/09', 381),\n",
       " ('2018/06/04', 381),\n",
       " ('2018/06/21', 381),\n",
       " ('2019/06/22', 381),\n",
       " ('2018/05/29', 381),\n",
       " ('2019/10/07', 380),\n",
       " ('2018/04/15', 380),\n",
       " ('2019/05/08', 380),\n",
       " ('2018/08/26', 380),\n",
       " ('2018/03/05', 379),\n",
       " ('2018/06/18', 379),\n",
       " ('2019/03/31', 379),\n",
       " ('2018/12/06', 379),\n",
       " ('2018/11/16', 379),\n",
       " ('2019/10/31', 379),\n",
       " ('2019/12/18', 379),\n",
       " ('2019/12/02', 378),\n",
       " ('2018/11/25', 378),\n",
       " ('2019/03/11', 378),\n",
       " ('2019/07/18', 378),\n",
       " ('2018/11/04', 377),\n",
       " ('2019/02/27', 377),\n",
       " ('2018/11/06', 377),\n",
       " ('2019/04/20', 377),\n",
       " ('2018/04/12', 377),\n",
       " ('2019/10/27', 377),\n",
       " ('2019/04/30', 377),\n",
       " ('2018/10/24', 377),\n",
       " ('2018/12/30', 376),\n",
       " ('2019/03/25', 376),\n",
       " ('2018/01/21', 375),\n",
       " ('2018/02/19', 375),\n",
       " ('2019/04/13', 375),\n",
       " ('2019/12/23', 375),\n",
       " ('2019/01/03', 374),\n",
       " ('2019/02/06', 374),\n",
       " ('2019/01/02', 374),\n",
       " ('2019/11/27', 374),\n",
       " ('2019/05/09', 374),\n",
       " ('2019/01/24', 373),\n",
       " ('2019/12/07', 373),\n",
       " ('2019/05/23', 372),\n",
       " ('2019/04/26', 372),\n",
       " ('2019/06/08', 372),\n",
       " ('2019/11/24', 372),\n",
       " ('2019/02/04', 371),\n",
       " ('2019/05/11', 371),\n",
       " ('2018/04/21', 371),\n",
       " ('2019/06/13', 370),\n",
       " ('2018/03/21', 370),\n",
       " ('2018/01/16', 370),\n",
       " ('2019/09/08', 370),\n",
       " ('2019/01/28', 370),\n",
       " ('2018/04/23', 369),\n",
       " ('2019/05/16', 369),\n",
       " ('2019/07/06', 369),\n",
       " ('2018/09/30', 369),\n",
       " ('2019/02/16', 369),\n",
       " ('2019/02/10', 368),\n",
       " ('2018/12/10', 368),\n",
       " ('2018/11/17', 368),\n",
       " ('2019/04/06', 368),\n",
       " ('2019/05/12', 368),\n",
       " ('2019/06/02', 367),\n",
       " ('2019/12/16', 367),\n",
       " ('2018/09/13', 367),\n",
       " ('2019/04/12', 366),\n",
       " ('2019/11/23', 366),\n",
       " ('2018/04/22', 365),\n",
       " ('2018/06/03', 365),\n",
       " ('2019/01/17', 365),\n",
       " ('2018/10/29', 365),\n",
       " ('2019/04/09', 364),\n",
       " ('2019/01/27', 363),\n",
       " ('2019/07/04', 363),\n",
       " ('2018/04/29', 363),\n",
       " ('2019/06/23', 363),\n",
       " ('2019/08/04', 362),\n",
       " ('2019/02/13', 362),\n",
       " ('2018/09/06', 362),\n",
       " ('2019/11/26', 362),\n",
       " ('2018/05/07', 362),\n",
       " ('2018/02/13', 362),\n",
       " ('2019/03/20', 362),\n",
       " ('2019/01/12', 362),\n",
       " ('2019/02/23', 361),\n",
       " ('2018/07/08', 361),\n",
       " ('2019/11/30', 361),\n",
       " ('2019/02/11', 360),\n",
       " ('2019/04/02', 360),\n",
       " ('2018/08/19', 360),\n",
       " ('2019/03/02', 360),\n",
       " ('2019/07/14', 359),\n",
       " ('2019/02/18', 359),\n",
       " ('2019/03/14', 358),\n",
       " ('2019/12/08', 358),\n",
       " ('2019/12/14', 358),\n",
       " ('2019/12/21', 356),\n",
       " ('2019/02/07', 356),\n",
       " ('2019/12/20', 355),\n",
       " ('2019/11/29', 354),\n",
       " ('2018/11/18', 354),\n",
       " ('2019/10/13', 353),\n",
       " ('2018/05/13', 353),\n",
       " ('2019/03/04', 353),\n",
       " ('2019/11/03', 353),\n",
       " ('2018/02/11', 352),\n",
       " ('2018/10/14', 352),\n",
       " ('2019/04/28', 351),\n",
       " ('2019/12/27', 350),\n",
       " ('2018/04/08', 350),\n",
       " ('2019/12/10', 350),\n",
       " ('2019/01/15', 349),\n",
       " ('2019/12/19', 349),\n",
       " ('2018/06/17', 349),\n",
       " ('2018/12/11', 349),\n",
       " ('2018/03/26', 348),\n",
       " ('2019/01/13', 348),\n",
       " ('2019/03/27', 348),\n",
       " ('2018/01/08', 348),\n",
       " ('2019/05/04', 347),\n",
       " ('2018/03/06', 347),\n",
       " ('2019/03/10', 346),\n",
       " ('2019/05/27', 346),\n",
       " ('2019/04/14', 345),\n",
       " ('2019/04/29', 342),\n",
       " ('2018/06/07', 342),\n",
       " ('2018/12/02', 342),\n",
       " ('2019/07/09', 339),\n",
       " ('2019/01/07', 339),\n",
       " ('2019/03/23', 339),\n",
       " ('2019/06/16', 338),\n",
       " ('2019/02/24', 337),\n",
       " ('2019/03/13', 334),\n",
       " ('2019/03/24', 333),\n",
       " ('2019/02/17', 333),\n",
       " ('2019/11/18', 333),\n",
       " ('2019/03/03', 332),\n",
       " ('2019/12/01', 332),\n",
       " ('2019/01/16', 332),\n",
       " ('2019/03/05', 331),\n",
       " ('2019/05/02', 331),\n",
       " ('2019/12/17', 329),\n",
       " ('2018/02/04', 329),\n",
       " ('2019/03/17', 329),\n",
       " ('2019/02/14', 326),\n",
       " ('2019/02/03', 326),\n",
       " ('2019/04/21', 325),\n",
       " ('2019/04/25', 324),\n",
       " ('2019/12/15', 321),\n",
       " ('2018/11/27', 318),\n",
       " ('2019/12/28', 317),\n",
       " ('2019/06/09', 315),\n",
       " ('2019/02/26', 313),\n",
       " ('2018/11/22', 311),\n",
       " ('2019/03/09', 310),\n",
       " ('2019/12/24', 308),\n",
       " ('2019/12/22', 307),\n",
       " ('2019/05/21', 306),\n",
       " ('2018/12/16', 302),\n",
       " ('2019/12/30', 295),\n",
       " ('2019/12/26', 293),\n",
       " ('2019/01/20', 292),\n",
       " ('2018/03/11', 287),\n",
       " ('2019/01/06', 272),\n",
       " ('2019/12/29', 262),\n",
       " ('2018/12/25', 259),\n",
       " ('2019/11/28', 239),\n",
       " ('2019/12/25', 223),\n",
       " ('2019/12/31', 221),\n",
       " ('2020/01/01', 217)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "catorgory_set_rdd = crimes.map(lambda item: (item[1],1))\n",
    "from operator import add\n",
    "result = sorted(catorgory_set_rdd.reduceByKey(add).collect(), key = lambda item: -item[1])\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Category: string, count: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q1_result = df_opt1.groupBy('Category').count().orderBy('count', ascending=False)\n",
    "display(q1_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+------+\n",
      "|Category                   |count |\n",
      "+---------------------------+------+\n",
      "|LARCENY/THEFT              |174900|\n",
      "|OTHER OFFENSES             |126182|\n",
      "|NON-CRIMINAL               |92304 |\n",
      "|ASSAULT                    |76876 |\n",
      "|DRUG/NARCOTIC              |53971 |\n",
      "|VEHICLE THEFT              |53781 |\n",
      "|VANDALISM                  |44725 |\n",
      "|WARRANTS                   |42214 |\n",
      "|BURGLARY                   |36755 |\n",
      "|SUSPICIOUS OCC             |31414 |\n",
      "|MISSING PERSON             |25989 |\n",
      "|ROBBERY                    |23000 |\n",
      "|FRAUD                      |16679 |\n",
      "|FORGERY/COUNTERFEITING     |10609 |\n",
      "|SECONDARY CODES            |9985  |\n",
      "|WEAPON LAWS                |8555  |\n",
      "|PROSTITUTION               |7484  |\n",
      "|TRESPASS                   |7326  |\n",
      "|STOLEN PROPERTY            |4540  |\n",
      "|SEX OFFENSES FORCIBLE      |4388  |\n",
      "|DISORDERLY CONDUCT         |4320  |\n",
      "|DRUNKENNESS                |4280  |\n",
      "|RECOVERED VEHICLE          |3138  |\n",
      "|KIDNAPPING                 |2341  |\n",
      "|DRIVING UNDER THE INFLUENCE|2268  |\n",
      "|RUNAWAY                    |1946  |\n",
      "|LIQUOR LAWS                |1903  |\n",
      "|ARSON                      |1513  |\n",
      "|LOITERING                  |1225  |\n",
      "|EMBEZZLEMENT               |1166  |\n",
      "+---------------------------+------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1_result.show(n = 30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[category: string, Count: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Spark SQL based\n",
    "crimeCategory = spark.sql(\"SELECT  category, COUNT(*) AS Count FROM sf_crime GROUP BY category ORDER BY count DESC\")\n",
    "display(crimeCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|      category| Count|\n",
      "+--------------+------+\n",
      "| LARCENY/THEFT|174900|\n",
      "|OTHER OFFENSES|126182|\n",
      "|  NON-CRIMINAL| 92304|\n",
      "|       ASSAULT| 76876|\n",
      "| DRUG/NARCOTIC| 53971|\n",
      "| VEHICLE THEFT| 53781|\n",
      "|     VANDALISM| 44725|\n",
      "|      WARRANTS| 42214|\n",
      "|      BURGLARY| 36755|\n",
      "|SUSPICIOUS OCC| 31414|\n",
      "+--------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crimeCategory.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important hints: \n",
    "## first step: spark df or sql to compute the statisitc result \n",
    "## second step: export your result to a pandas dataframe. \n",
    "crimes_pd_df = crimeCategory.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>174900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>126182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NON-CRIMINAL</td>\n",
       "      <td>92304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ASSAULT</td>\n",
       "      <td>76876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>DRUG/NARCOTIC</td>\n",
       "      <td>53971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>VEHICLE THEFT</td>\n",
       "      <td>53781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>VANDALISM</td>\n",
       "      <td>44725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>42214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>BURGLARY</td>\n",
       "      <td>36755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>SUSPICIOUS OCC</td>\n",
       "      <td>31414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>MISSING PERSON</td>\n",
       "      <td>25989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>ROBBERY</td>\n",
       "      <td>23000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>FRAUD</td>\n",
       "      <td>16679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>FORGERY/COUNTERFEITING</td>\n",
       "      <td>10609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>SECONDARY CODES</td>\n",
       "      <td>9985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>WEAPON LAWS</td>\n",
       "      <td>8555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>PROSTITUTION</td>\n",
       "      <td>7484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>TRESPASS</td>\n",
       "      <td>7326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>STOLEN PROPERTY</td>\n",
       "      <td>4540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>SEX OFFENSES FORCIBLE</td>\n",
       "      <td>4388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>DISORDERLY CONDUCT</td>\n",
       "      <td>4320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>DRUNKENNESS</td>\n",
       "      <td>4280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>RECOVERED VEHICLE</td>\n",
       "      <td>3138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>KIDNAPPING</td>\n",
       "      <td>2341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>DRIVING UNDER THE INFLUENCE</td>\n",
       "      <td>2268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>RUNAWAY</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>LIQUOR LAWS</td>\n",
       "      <td>1903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>ARSON</td>\n",
       "      <td>1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>LOITERING</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>EMBEZZLEMENT</td>\n",
       "      <td>1166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>SUICIDE</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>FAMILY OFFENSES</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>BAD CHECKS</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>BRIBERY</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>EXTORTION</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>SEX OFFENSES NON FORCIBLE</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>GAMBLING</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>PORNOGRAPHY/OBSCENE MAT</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>TREA</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       category   Count\n",
       "0                 LARCENY/THEFT  174900\n",
       "1                OTHER OFFENSES  126182\n",
       "2                  NON-CRIMINAL   92304\n",
       "3                       ASSAULT   76876\n",
       "4                 DRUG/NARCOTIC   53971\n",
       "5                 VEHICLE THEFT   53781\n",
       "6                     VANDALISM   44725\n",
       "7                      WARRANTS   42214\n",
       "8                      BURGLARY   36755\n",
       "9                SUSPICIOUS OCC   31414\n",
       "10               MISSING PERSON   25989\n",
       "11                      ROBBERY   23000\n",
       "12                        FRAUD   16679\n",
       "13       FORGERY/COUNTERFEITING   10609\n",
       "14              SECONDARY CODES    9985\n",
       "15                  WEAPON LAWS    8555\n",
       "16                 PROSTITUTION    7484\n",
       "17                     TRESPASS    7326\n",
       "18              STOLEN PROPERTY    4540\n",
       "19        SEX OFFENSES FORCIBLE    4388\n",
       "20           DISORDERLY CONDUCT    4320\n",
       "21                  DRUNKENNESS    4280\n",
       "22            RECOVERED VEHICLE    3138\n",
       "23                   KIDNAPPING    2341\n",
       "24  DRIVING UNDER THE INFLUENCE    2268\n",
       "25                      RUNAWAY    1946\n",
       "26                  LIQUOR LAWS    1903\n",
       "27                        ARSON    1513\n",
       "28                    LOITERING    1225\n",
       "29                 EMBEZZLEMENT    1166\n",
       "30                      SUICIDE     508\n",
       "31              FAMILY OFFENSES     491\n",
       "32                   BAD CHECKS     406\n",
       "33                      BRIBERY     289\n",
       "34                    EXTORTION     256\n",
       "35    SEX OFFENSES NON FORCIBLE     148\n",
       "36                     GAMBLING     146\n",
       "37      PORNOGRAPHY/OBSCENE MAT      22\n",
       "38                         TREA       6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spark does not support this function, please refer https://matplotlib.org/ for visuliation. You need to use display to show the figure in the databricks community. \n",
    "display(crimes_pd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see the top-5 frequency crimes are:\n",
    "1\tLARCENY/THEFT <br>\n",
    "2\tOTHER OFFENSES <br>\n",
    "3\tNON-CRIMINAL <br>\n",
    "4\tASSAULT <br>\n",
    "5\tDRUG/NARCOTIC <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 question (OLAP)\n",
    "Counts the number of crimes for different district, and visualize your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PdDistrict: string, Count: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crimeDistrict = spark.sql(\"SELECT  PdDistrict, COUNT(*) AS Count FROM sf_crime GROUP BY PdDistrict ORDER BY Count DESC\")\n",
    "display(crimeDistrict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|PdDistrict| Count|\n",
      "+----------+------+\n",
      "|  SOUTHERN|157182|\n",
      "|   MISSION|119908|\n",
      "|  NORTHERN|105296|\n",
      "|   BAYVIEW| 89431|\n",
      "|   CENTRAL| 85460|\n",
      "|TENDERLOIN| 81809|\n",
      "| INGLESIDE| 78845|\n",
      "|   TARAVAL| 65596|\n",
      "|      PARK| 49313|\n",
      "|  RICHMOND| 45209|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crimeDistrict.show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Southern and Mission distric are most dangerous, there are more than 100K crimes committed between 2003 to 2015. <br>\n",
    "\n",
    "Compared to other districts, Park and RichMond are public security environmental. <br>\n",
    "\n",
    "From the map, we can see Northern districs are safer than Southern ones. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 question (OLAP)\n",
    "Count the number of crimes each \"Sunday\" at \"SF downtown\". hints: SF downtown is defiend via the range of spatial location. Thus, you need to write your own UDF function to filter data which are located inside certain spatial range. You can follow the example here: https://changhsinlee.com/pyspark-udf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PdDistrict = spark.sql(\"SELECT distinct PdDistrict FROM sf_crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|PdDistrict|\n",
      "+----------+\n",
      "|   MISSION|\n",
      "|   BAYVIEW|\n",
      "|   CENTRAL|\n",
      "|   TARAVAL|\n",
      "|TENDERLOIN|\n",
      "| INGLESIDE|\n",
      "|      PARK|\n",
      "|  SOUTHERN|\n",
      "|  RICHMOND|\n",
      "|  NORTHERN|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PdDistrict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##sql\n",
    "crimecount = spark.sql(\"SELECT  PdDistrict, DayOfWeek, COUNT(*) AS Count FROM sf_crime WHERE PdDistrict in ('MISSION','CENTRAL','TARAVAL','TENDERLOIN','RICHMOND','PARK') and DayOfWeek='Sunday' GROUP BY PdDistrict,DayOfWeek ORDER BY Count DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|PdDistrict|DayOfWeek|Count|\n",
      "+----------+---------+-----+\n",
      "|   MISSION|   Sunday|15874|\n",
      "|   CENTRAL|   Sunday|12197|\n",
      "|TENDERLOIN|   Sunday|10178|\n",
      "|   TARAVAL|   Sunday| 8331|\n",
      "|      PARK|   Sunday| 6646|\n",
      "|  RICHMOND|   Sunday| 6089|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crimecount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>MISSION</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>15874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CENTRAL</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>12197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>TENDERLOIN</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>10178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>TARAVAL</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>8331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>PARK</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>6646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>RICHMOND</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>6089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PdDistrict DayOfWeek  Count\n",
       "0     MISSION    Sunday  15874\n",
       "1     CENTRAL    Sunday  12197\n",
       "2  TENDERLOIN    Sunday  10178\n",
       "3     TARAVAL    Sunday   8331\n",
       "4        PARK    Sunday   6646\n",
       "5    RICHMOND    Sunday   6089"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crimecount_pd=crimecount.toPandas()\n",
    "display(crimecount_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4 question (OLAP)\n",
    "Analysis the number of crime in each month of 2015, 2016, 2017, 2018. Then, give your insights for the output results. What is the business impact for your result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pyspark.sql.functions as pysparkSqlFunc\n",
    "func = pysparkSqlFunc.udf(lambda x: datetime.strptime(x, '%m/%d/%Y'), DateType())\n",
    "df = df_opt3.withColumn('new_date', func(col('Dates')))\n",
    "df.createOrReplaceTempView('sf_crime1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Dates: string, Category: string, Descript: string, DayOfWeek: string, PdDistrict: string, Resolution: string, Address: string, X: string, Y: string, new_date: date]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, count(1): bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##sql\n",
    "Monthcrime = spark.sql(\"SELECT YEAR(new_date) as year, Month(new_date) as month, Count(*) from sf_crime1 group by YEAR(new_date), Month(new_date) order by 1,2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, count(1): bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Monthcrime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5 question (OLAP)\n",
    "Analysis the number of crime w.r.t the hour in certian day like 2015/12/15, 2016/12/15, 2017/12/15, 2018/10/15. Then, give your travel suggestion to visit SF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`Time`' given input columns: [DayOfWeek, Category, Dates, X, Address, PdDistrict, Y, Resolution, Descript];;\\n'Project [Dates#113, Category#114, Descript#115, DayOfWeek#116, PdDistrict#117, Resolution#118, Address#119, X#120, Y#121, <lambda>('Time) AS new_time#475]\\n+- LogicalRDD [Dates#113, Category#114, Descript#115, DayOfWeek#116, PdDistrict#117, Resolution#118, Address#119, X#120, Y#121], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o132.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Time`' given input columns: [DayOfWeek, Category, Dates, X, Address, PdDistrict, Y, Resolution, Descript];;\n'Project [Dates#113, Category#114, Descript#115, DayOfWeek#116, PdDistrict#117, Resolution#118, Address#119, X#120, Y#121, <lambda>('Time) AS new_time#475]\n+- LogicalRDD [Dates#113, Category#114, Descript#115, DayOfWeek#116, PdDistrict#117, Resolution#118, Address#119, X#120, Y#121], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:285)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:84)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:84)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3301)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1312)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2197)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2164)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-57237c8b53c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfunc1\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpysparkSqlFunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mudf\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%H:%M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimestampType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_opt3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new_time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sf_crime2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \"\"\"\n\u001b[1;32m   1848\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`Time`' given input columns: [DayOfWeek, Category, Dates, X, Address, PdDistrict, Y, Resolution, Descript];;\\n'Project [Dates#113, Category#114, Descript#115, DayOfWeek#116, PdDistrict#117, Resolution#118, Address#119, X#120, Y#121, <lambda>('Time) AS new_time#475]\\n+- LogicalRDD [Dates#113, Category#114, Descript#115, DayOfWeek#116, PdDistrict#117, Resolution#118, Address#119, X#120, Y#121], false\\n\""
     ]
    }
   ],
   "source": [
    "func1 =  pysparkSqlFunc.udf (lambda x: datetime.strptime(x, '%H:%M'), TimestampType())\n",
    "df2 = df_opt3.withColumn('new_time', func1(col('Time')))\n",
    "df2.createOrReplaceTempView('sf_crime2')\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For different category of crime, find the percentage of resolution. Based on the output, give your hints to adjust the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PdDistrict: string, Resolution: string, CT: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resolution=spark.sql('select PdDistrict, Resolution, count(*) as CT from sf_crime1 group by 1,2 order by 1,2')\n",
    "resolution.createOrReplaceTempView('resolution')\n",
    "\n",
    "display(resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+\n",
      "|PdDistrict|          Resolution|   CT|\n",
      "+----------+--------------------+-----+\n",
      "|   BAYVIEW|      ARREST, BOOKED|19492|\n",
      "|   BAYVIEW|       ARREST, CITED| 9795|\n",
      "|   BAYVIEW|CLEARED-CONTACT J...|   42|\n",
      "|   BAYVIEW|COMPLAINANT REFUS...|  678|\n",
      "|   BAYVIEW|DISTRICT ATTORNEY...|  659|\n",
      "|   BAYVIEW|EXCEPTIONAL CLEAR...|  128|\n",
      "|   BAYVIEW| JUVENILE ADMONISHED|  177|\n",
      "|   BAYVIEW|     JUVENILE BOOKED|  855|\n",
      "|   BAYVIEW|      JUVENILE CITED|  352|\n",
      "|   BAYVIEW|   JUVENILE DIVERTED|   74|\n",
      "|   BAYVIEW|             LOCATED| 3180|\n",
      "|   BAYVIEW|                NONE|51785|\n",
      "|   BAYVIEW|      NOT PROSECUTED|  280|\n",
      "|   BAYVIEW|PROSECUTED BY OUT...|  186|\n",
      "|   BAYVIEW|PROSECUTED FOR LE...|   11|\n",
      "|   BAYVIEW|   PSYCHOPATHIC CASE|  806|\n",
      "|   BAYVIEW|           UNFOUNDED|  931|\n",
      "|   CENTRAL|      ARREST, BOOKED|13830|\n",
      "|   CENTRAL|       ARREST, CITED| 5964|\n",
      "|   CENTRAL|CLEARED-CONTACT J...|    9|\n",
      "+----------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resolution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+------------------+\n",
      "|PdDistrict|solved|unsolved|percentageofsolved|\n",
      "+----------+------+--------+------------------+\n",
      "|TENDERLOIN| 81809|   28406|0.7422673864718958|\n",
      "|   MISSION|119908|   64336|0.6508108812227263|\n",
      "|   BAYVIEW| 89431|   52716| 0.629144477196142|\n",
      "|  SOUTHERN|157182|   95504|0.6220447511931805|\n",
      "|      PARK| 49313|   31394|0.6110126754804416|\n",
      "| INGLESIDE| 78845|   51939|0.6028642647418645|\n",
      "|  NORTHERN|105296|   71344|0.5961050724637681|\n",
      "|   TARAVAL| 65596|   45785|0.5889334805756816|\n",
      "|   CENTRAL| 85460|   61458|0.5816850215766618|\n",
      "|  RICHMOND| 45209|   33493|0.5744326700719169|\n",
      "+----------+------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resolution2=spark.sql('select PdDistrict, solved, unsolved, solved/(solved+unsolved) as percentageofsolved from (select PdDistrict, sum(case when Resolution in (\"NONE\",\"UNFOUNDED\") then CT else 0 end) as unsolved, sum(case when PdDistrict not in (\"NONE\",\"UNFOUNDED\") then CT else 0 end) as solved from resolution group by PdDistrict) order by percentageofsolved DESC')\n",
    "resolution2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8 question (Apply Spark ML clustering for spatial data analysis)\n",
    "Extra: visualize the spatial distribution of crimes and run a kmeans clustering algorithm (please use Spark ML kmeans)\n",
    "You can refer Spark ML Kmeans a example: https://spark.apache.org/docs/latest/ml-clustering.html#k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`Count`' given input columns: [Address, Descript, DayOfWeek, Category, X, Y, Resolution, PdDistrict, Dates];;\\n'Project ['Count, X#17, Y#18]\\n+- Relation[Dates#10,Category#11,Descript#12,DayOfWeek#13,PdDistrict#14,Resolution#15,Address#16,X#17,Y#18] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o76.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Count`' given input columns: [Address, Descript, DayOfWeek, Category, X, Y, Resolution, PdDistrict, Dates];;\n'Project ['Count, X#17, Y#18]\n+- Relation[Dates#10,Category#11,Descript#12,DayOfWeek#13,PdDistrict#14,Resolution#15,Address#16,X#17,Y#18] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:84)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:84)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3301)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1312)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-8b39f28e969b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#loads data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf8\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_opt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mFeature_Col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \"\"\"\n\u001b[0;32m-> 1202\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`Count`' given input columns: [Address, Descript, DayOfWeek, Category, X, Y, Resolution, PdDistrict, Dates];;\\n'Project ['Count, X#17, Y#18]\\n+- Relation[Dates#10,Category#11,Descript#12,DayOfWeek#13,PdDistrict#14,Resolution#15,Address#16,X#17,Y#18] csv\\n\""
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "#loads data\n",
    "df8=df_opt1.select(['Count','X','Y'])\n",
    "Feature_Col=['X','Y']\n",
    "for col in df8.columns:\n",
    "  if col in Feature_Col:\n",
    "    df8=df8.withColumn(col,df8[col].cast('float'))\n",
    "vecAssembler=VectorAssembler(inputCols=Feature_Col,outputCol=\"features\")\n",
    "df_kmeans=vecAssembler.transform(df8).select('IncidntNum','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
